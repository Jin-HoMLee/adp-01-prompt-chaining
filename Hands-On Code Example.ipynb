{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f10547",
   "metadata": {},
   "source": [
    "# Hands-On Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b420faf",
   "metadata": {},
   "source": [
    "> Adapted and modified from https://docs.google.com/document/d/1flxKGrbnF2g8yh3F-oVD5Xx7ZumId56HbFpIiPdkqLI/edit?tab=t.0 \n",
    "> \n",
    "> Fr  5 Sep 2025 10:41:12 CEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40024e8",
   "metadata": {},
   "source": [
    "Implementing prompt chaining ranges from direct, sequential function calls within a script to the utilization of specialized frameworks designed to manage control flow, state, and component integration. Frameworks such as LangChain, LangGraph, Crew AI, and the Google Agent Development Kit (ADK) offer structured environments for constructing and executing these multi-step processes, which is particularly advantageous for complex architectures.\n",
    "\n",
    "For the purpose of demonstration, LangChain and LangGraph are suitable choices as their core APIs are explicitly designed for composing chains and graphs of operations. LangChain provides foundational abstractions for linear sequences, while LangGraph extends these capabilities to support stateful and cyclical computations, which are necessary for implementing more sophisticated agentic behaviors. This example will focus on a fundamental linear sequence.\n",
    "\n",
    "The following code implements a two-step prompt chain that functions as a data processing pipeline. The initial stage is designed to parse unstructured text and extract specific information. The subsequent stage then receives this extracted output and transforms it into a structured data format.\n",
    "\n",
    "To replicate this procedure, the required libraries must first be installed. This can be accomplished using the following command: \n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-openai langgraph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279296f",
   "metadata": {},
   "source": [
    "Note that langchain-openai can be substituted with the appropriate package for a different model provider. Subsequently, the execution environment must be configured with the necessary API credentials for the selected language model provider, such as OpenAI, Google Gemini, or Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65aff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# For better security, load environment variables from a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Make sure your OPENAI_API_KEY is set in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e95320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Language Model (using ChatOpenAI is recommended)\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5d1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt 1: Extract Information ---\n",
    "prompt_extract = ChatPromptTemplate.from_template(\n",
    "   \"Extract the technical specifications from the following text:\\n\\n{text_input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b674bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt 2: Transform to JSON ---\n",
    "prompt_transform = ChatPromptTemplate.from_template(\n",
    "   \"Transform the following specifications into a JSON object with 'cpu', 'memory', and 'storage' as keys:\\n\\n{specifications}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b459c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the Chain using LCEL ---\n",
    "# The StrOutputParser() converts the LLM's message output to a simple string.\n",
    "extraction_chain = prompt_extract | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "688c78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full chain passes the output of the extraction chain into the 'specifications'\n",
    "# variable for the transformation prompt.\n",
    "full_chain = (\n",
    "   {\"specifications\": extraction_chain}\n",
    "   | prompt_transform\n",
    "   | llm\n",
    "   | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1ae31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Chain ---\n",
    "input_text = \"The new laptop model features a 3.5 GHz octa-core processor, 16GB of RAM, and a 1TB NVMe SSD.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8701bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain with the input text dictionary.\n",
    "final_result = full_chain.invoke({\"text_input\": input_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe5ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final JSON Output ---\n",
      "{\n",
      "    \"cpu\": {\n",
      "        \"processor\": \"3.5 GHz octa-core\"\n",
      "    },\n",
      "    \"memory\": {\n",
      "        \"RAM\": \"16GB\"\n",
      "    },\n",
      "    \"storage\": {\n",
      "        \"Storage\": \"1TB NVMe SSD\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final JSON Output ---\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd6a51",
   "metadata": {},
   "source": [
    "This Python code demonstrates how to use the LangChain library to process text. It utilizes two separate prompts: one to extract technical specifications from an input string and another to format these specifications into a JSON object. The ChatOpenAI model is employed for language model interactions, and the StrOutputParser ensures the output is in a usable string format. The LangChain Expression Language (LCEL) is used to elegantly chain these prompts and the language model together. The first chain, extraction_chain, extracts the specifications. The full_chain then takes the output of the extraction and uses it as input for the transformation prompt. A sample input text describing a laptop is provided. The full_chain is invoked with this text, processing it through both steps. The final result, a JSON string containing the extracted and formatted specifications, is then printed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
